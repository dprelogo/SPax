{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import numpy as np\n",
    "import spax\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher information\n",
    "Let $p(\\mathbf{d} \\in D | \\boldsymbol{\\theta})$ be a pdf parameterized with $\\boldsymbol{\\theta}$. Fisher matrix is then defined as:\n",
    "$$\\mathcal{F}_{ij} = \\operatorname{E}\\left[\\left.\\left(\\frac{\\partial}{\\partial\\theta_i} \\log p(D|\\boldsymbol\\theta)\\right)\\left(\\frac{\\partial}{\\partial\\theta_j} \\log p(D|\\boldsymbol\\theta)\\right)\\right|\\boldsymbol\\theta\\right] = -\\operatorname{E}\\left[\\left.\\frac{\\partial^2}{\\partial\\theta_i\\, \\partial\\theta_j} \\log p(D|\\boldsymbol\\theta)\\right|\\boldsymbol\\theta\\right]\\,.$$\n",
    "\n",
    "## Gaussian-distributed data\n",
    "In the simplest case, we can assume multivariate gaussian as the underlying pdf, $\\log p(\\mathbf{d}|\\boldsymbol\\theta) = - 1/2\\,  (\\mathbf{d} - \\boldsymbol\\mu(\\boldsymbol\\theta))^\\textsf{T} \\, \\Sigma^{-1} \\, (\\mathbf{d} - \\boldsymbol\\mu(\\boldsymbol\\theta))$, where we assumed that covariance matrix doesn't depend on $\\boldsymbol\\theta$.\n",
    "\n",
    "Now from equation above it follows: $$ \\mathcal{F}_{ij} = \\frac{\\partial\\boldsymbol\\mu^\\textsf{T}}{\\partial\\theta_i}\\Sigma^{-1}\\frac{\\partial\\boldsymbol\\mu}{\\partial\\theta_j}$$\n",
    "If $X$ is $n \\times m$ matrix, where $n$ represents dimensionality of the data and $m$ number of samples, then covariance matrix is simply $$\\Sigma = \\frac{1}{m-1} X \\, X^T \\, ,$$\n",
    "with the assumption that the mean was removed, i.e. `X -= X.mean(axis = 1)`.\n",
    "\n",
    "On the other hand, if $X_i^{+}$ and $X_i^{-}$ are $n \\times m'$ matrices representing a set of data points with distance $\\Delta\\theta_i$ apart, then:\n",
    "$$\\frac{\\partial\\boldsymbol\\mu}{\\partial\\theta_i} \\approx \\frac{1}{m'}\\sum_k \\frac{\\mathbf{d}_{ki}^{+} - \\mathbf{d}_{ki}^{-}}{\\Delta\\theta_i} \\, ,$$\n",
    "where $\\mathbf{d}_{ki}^{+, -}$ are columns of $X_i^{+, -}$.\n",
    "\n",
    "## Need for compression\n",
    "In the case $n \\ge m$, covariance matrix is non invertible and some data compression is needed. Here we are implementing a simple PCA compression of order $N$: $ \\widetilde{X}_N = U_N^T \\, X$, where $\\sim$ denotes compressed space and $U_N^T$ is $n \\times N$ rotation matrix.\n",
    "\n",
    "Covariance matrix can now be written as\n",
    "$$ \\widetilde{\\Sigma}_N = \\frac{1}{m-1} \\widetilde{X}_N \\, \\widetilde{X}_N^T = \\frac{1}{m-1} U_N^T \\, X \\, X^T U_N = \\sigma_N^2 \\, ,$$\n",
    "where $\\sigma_N^2$ is diagonal matrix containing first $N$ principal components of the covariance matrix.\n",
    "\n",
    "If we denote $J \\equiv \\partial\\boldsymbol\\mu / \\partial\\boldsymbol\\theta$, one can easily show $\\widetilde{J}_N = U_N^T J$. Therefore, if we start with $F = J^T \\Sigma^{-1} J$, after PCA compression one has:\n",
    "$$ F = J^T \\, U_N  \\, \\sigma_N^{-2} \\, U_N^T J  \\, .$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}