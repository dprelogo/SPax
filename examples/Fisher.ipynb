{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/cluster/shared/software/libs/cuda/11a\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "import jax\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import spax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher information\n",
    "Let $p(\\mathbf{d} \\in D | \\boldsymbol{\\theta})$ be a pdf parameterized with $\\boldsymbol{\\theta}$. Fisher matrix is then defined as:\n",
    "$$\\mathcal{F}_{ij} = \\operatorname{E}\\left[\\left.\\left(\\frac{\\partial}{\\partial\\theta_i} \\log p(D|\\boldsymbol\\theta)\\right)\\left(\\frac{\\partial}{\\partial\\theta_j} \\log p(D|\\boldsymbol\\theta)\\right)\\right|\\boldsymbol\\theta\\right] = -\\operatorname{E}\\left[\\left.\\frac{\\partial^2}{\\partial\\theta_i\\, \\partial\\theta_j} \\log p(D|\\boldsymbol\\theta)\\right|\\boldsymbol\\theta\\right]\\,.$$\n",
    "\n",
    "## Gaussian-distributed data\n",
    "In the simplest case, we can assume multivariate gaussian as the underlying pdf, $\\log p(\\mathbf{d}|\\boldsymbol\\theta) = - 1/2\\,  (\\mathbf{d} - \\boldsymbol\\mu(\\boldsymbol\\theta))^\\textsf{T} \\, \\Sigma^{-1} \\, (\\mathbf{d} - \\boldsymbol\\mu(\\boldsymbol\\theta))$, where we assumed that covariance matrix doesn't depend on $\\boldsymbol\\theta$.\n",
    "\n",
    "Now from equation above it follows: $$ \\mathcal{F}_{ij} = \\frac{\\partial\\boldsymbol\\mu^\\textsf{T}}{\\partial\\theta_i}\\Sigma^{-1}\\frac{\\partial\\boldsymbol\\mu}{\\partial\\theta_j}$$\n",
    "If $X$ is $n \\times m$ matrix, where $n$ represents dimensionality of the data and $m$ number of samples, then covariance matrix is simply $$\\Sigma = \\frac{1}{m-1} X \\, X^T \\, ,$$\n",
    "with the assumption that the mean was removed, i.e. `X -= X.mean(axis = 1)`.\n",
    "\n",
    "On the other hand, if $X_i^{+}$ and $X_i^{-}$ are $n \\times m'$ matrices representing a set of data points with distance $\\Delta\\theta_i$ apart, then:\n",
    "$$\\frac{\\partial\\boldsymbol\\mu}{\\partial\\theta_i} \\approx \\frac{1}{m'}\\sum_k \\frac{\\mathbf{d}_{ki}^{+} - \\mathbf{d}_{ki}^{-}}{\\Delta\\theta_i} \\, ,$$\n",
    "where $\\mathbf{d}_{ki}^{+, -}$ are columns of $X_i^{+, -}$.\n",
    "\n",
    "## Need for compression\n",
    "In the case $n \\ge m$, covariance matrix is non invertible and some data compression is needed. Here we are implementing a simple PCA compression of order $N$: $ \\widetilde{X}_N = U_N^T \\, X$, where $\\sim$ denotes compressed space and $U_N$ is $n \\times N$ rotation matrix.\n",
    "\n",
    "Covariance matrix can now be written as\n",
    "$$ \\widetilde{\\Sigma}_N = \\frac{1}{m-1} \\widetilde{X}_N \\, \\widetilde{X}_N^T = \\frac{1}{m-1} U_N^T \\, X \\, X^T U_N = \\sigma_N^2 \\, ,$$\n",
    "where $\\sigma_N^2$ is diagonal matrix containing first $N$ principal components of the covariance matrix.\n",
    "\n",
    "If we denote $J \\equiv \\partial\\boldsymbol\\mu / \\partial\\boldsymbol\\theta$, one can easily show $\\widetilde{J}_N = U_N^T J$. Therefore, if we start with $F = J^T \\Sigma^{-1} J$, after PCA compression one has:\n",
    "$$ F_N = J^T \\, U_N  \\, \\sigma_N^{-2} \\, U_N^T J  \\, .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extras\n",
    "$$ X_N = U_N \\, U_N^T \\, X$$\n",
    "$$ \\Sigma_N \\equiv \\frac{1}{m-1} \\, X_N \\, X_N^T = \\frac{1}{m-1} \\, U_N \\, U_N^T \\, X \\, X^T \\, U_N \\, U_N^T = U_N \\, \\sigma_N^2 \\, U_N^T $$\n",
    "$$ \\Sigma_N^{-1} \\equiv U_N \\, \\sigma_N^{-2} \\, U_N^T $$\n",
    "With this in mind and the fact that $U_N^T \\, U_N = I$, we can see that \n",
    "$$ \\Sigma_N^{-1} \\Sigma_N \\Sigma_N^{-1} = \\Sigma_N^{-1} \\, ,$$ \n",
    "$$ \\Sigma_N \\Sigma_N^{-1} \\Sigma_N = \\Sigma_N  \\, ,$$ \n",
    "i.e. it represents Moore-Penrose inverse. Moreover,\n",
    "$$ F_N = J^T \\, \\Sigma_N^{-1} \\, J \\, .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DeviceArray(-0., dtype=float32), DeviceArray(0., dtype=float32), DeviceArray(-0., dtype=float32), DeviceArray(-0., dtype=float32), DeviceArray(-0., dtype=float32), DeviceArray(0., dtype=float32), DeviceArray(-0., dtype=float32), DeviceArray(-0., dtype=float32), DeviceArray(-0., dtype=float32), DeviceArray(0., dtype=float32), DeviceArray(0., dtype=float32), DeviceArray(-0., dtype=float32), DeviceArray(0., dtype=float32), DeviceArray(-0., dtype=float32), DeviceArray(-0., dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#constructing the data, here a 16-dim multivariate gaussian\n",
    "N_dim, N_samples = 16, 10000\n",
    "random_sample = np.random.normal(0, 1, size = (N_dim, N_samples))\n",
    "sigmas = np.arange(1, N_dim + 1)[:, np.newaxis]\n",
    "data =  random_sample * sigmas\n",
    "δθ = sigmas * 0.01\n",
    "derivative = np.empty((N_dim, 2, N_dim, N_samples))\n",
    "derivative[:, 0, ...], derivative[:, 1, ...] = random_sample * (sigmas - δθ / 2), random_sample * (sigmas + δθ / 2)\n",
    "\n",
    "Fisher = spax.Fisher()\n",
    "Fisher.fit(data, derivative, δθ, batch_size = 4)\n",
    "F = []\n",
    "for n in range(1, N_dim):\n",
    "    F.append(Fisher.compute(N = n))\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
